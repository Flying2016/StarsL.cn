title=Machine Learning for Interview
en_title=ML
category=MachineLearning
tags=MachineLearning,Interview
summary=为了找工作（算法方向），看了别的大神的文章，自己也看了一下李航的《统计学习方法》，写一个总结，主要是确保自己已经掌握这部分的内容，当然也分享给各位大侠，与大家共勉。
<ep_info>
#机器学习面试（算法方向）总结
下面给各位大侠总结一下最近面试的机器学习面试的内容以及自己在夯实机器学习这部分内容的时候的一些经验。

#一、朴素贝叶斯
朴素贝叶斯，为何叫朴素呢，英语为naive，也就是他的这个假设太强了，假设每个特征之间是相互独立的，但是在数据挖掘提特征的时候，很难做到特征的相互独立。虽然是这样，但是朴素贝叶斯还是以`对小规模的数据表现很好，适合多分类任务，适合增量式训练。`的优点有着广泛的应用。

>##1. 贝叶斯公式 & 贝叶斯定理
>
>$$P(A|B) = \frac{P(AB)}{P(B)}$$
>
>$$P(B|A)P(A) = P(A|B)P(B)$$

>##2. 工作原理
>>a. 样本
>>$$x = (a_1, a_2, a_3, a_4 .... a_n)$$
>
>>b. 分类目标
>>$$Y =  (y_1, y_2, y_3, y_4 .... y_n)$$
>
>>c. 最后的分类为
>>$$max(P(y_1|x), P(y_2|x), P(y_3|x) ... P(y_n|x))$$
>
>>d. 由于每个特征是相互独立的，所以依据贝叶斯公式
>>$$P(x|y_i)* p(y_i) = p(y_i)*\prod_i(P(a_i|y_i))$$

>##3. Tips
>a.使用训练样本在各个特征上的概率进行估计
>
>b.离散变量服从泊松分布，连续变量服从高斯分布
>
>c.当某个特征在样本中的出现的次数为零时，使用拉普拉斯平滑，在没有出现的类别+1就行了 

#二、逻辑回归(logistic regression)
`LR`使用的主要思路跟nn的思想长不多，训练`LR`时也就是在训练线性和函数的各个权重值`w`, 然后使用激活函数`sigmoid`，对和进行激活。
>##1.观测概率
>假设现在有样本{xi,yi},其中xi表示样本的特征，yi∈{0,1}表示样本的分类真实值，yi=1的概率是pi,则yi=0的概率是1−pi，那么观测概率为:
>
>$$p(y_i) = p_i^{y_i}*(1-p_i)^{1 -y_i}$$
>
>极大似然估计为
>
>$$L(w) = log (\prod (h_w(x_i)^{y_i}*(1-h_w(x_i))^{1-y_i}))$$
>
>$$L(w) = \sum _i\left(y_i*(w^Tx_i) - log(1+e^{w^tx_i}) \right)$$

>##2. 优化w
>a. `SGD`随机梯度下降
>>延梯度下降的反方向进行最小化似然估计。但是如果使用全部的样本就容易陷入局部最优解，所以一般随机会选取样本中的一个进行梯度下降。
>
>b. 其他优化方法
>
>>`拟牛顿法`、`BFGS`、`L-BFGS`（这些小w也不是很了解，待后面详细分解）
>##3. 多分类
>修改`LR`的`sigmoid`损失函数为`softmax`，Y的取值集合为{1,2,3 ... k}所以多分类的损失函数为
>
>$$P(Y=a|x) = \frac{exp(w_a*x)}{\sum_{i=1}^k exp(w_i*x)}; 1<a<k$$
>##4.k分类 `softmax`还是 k个`LR`
>如果k个分类是互不兼容的，那么使用`softmax`分类，但是如果可能是相互兼容的，那么就要选择k个`LR`